# k8s1.23.0高可用安装

## 准备工作

以下是服务器准备

```shell
192.168.1.10   Vip
192.168.1.20   master1
192.168.1.21   master2
192.168.1.22   node1
192.168.1.23   node2
```

每台机器修改hosts

```shell
cat >> /etc/hosts << EOF
192.168.1.20   master1
192.168.1.21   master2
192.168.1.22   node1
192.168.1.23   node2
EOF
```

然后关闭防火墙、关闭selinux、关闭swap（所有节点上执行）：

```shell
systemctl stop firewalld && systemctl disable firewalld
sed -i 's/enforcing/disabled/' /etc/selinux/config && setenforce 0
swapoff -a && sed -ri 's/.*swap.*/#&/' /etc/fstab
```

安装、设置ipvs

```shell
yum -y install ipvsadm ipset
```

创建ipvs设置脚本：

```shell
cat > /etc/sysconfig/modules/ipvs.modules << EOF
#!/bin/bash
modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack_ipv4
EOF

##执行脚本，验证修改结果：
chmod 755 /etc/sysconfig/modules/ipvs.modules
bash /etc/sysconfig/modules/ipvs.modules
lsmod | grep -e ip_vs -e nf_conntrack_ipv4
```

## 安装docker（所有节点安装）

```shell
#安装需要的软件包
yum install -y yum-utils device-mapper-persistent-data lvm2
#添加docker源
yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
#安装docker
yum install docker-ce -y && systemctl enable docker.service
#设置docker的驱动，和k8s的驱动保持一致，也可以在里面配置你自己的镜像仓库添加参数”insecure-registries”（离线部署k8s时最好配置仓库）
cat > /etc/docker/daemon.json <<EOF
{
  "registry-mirrors": ["http://f1361db2.m.daocloud.io",
                      "https://registry.docker-cn.com",
                      "http://hub-mirror.c.163.com",
                      "https://docker.mirrors.ustc.edu.cn"
                      ],
  "exec-opts": ["native.cgroupdriver=systemd"],
  "log-driver": "json-file",
  "log-opts": {"max-size": "100m"
  },
  "storage-driver": "overlay2"
}
EOF
#进行时间同步（所有节点执行）
yum install ntpdate -y && ntpdate time.windows.com
#配置内核参数，将桥接的IPv4流量传递到iptables的链：
cat > /etc/sysctl.d/k8s.conf <<EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF

#使配置的内核参数生效
sysctl -p
sysctl --system
```

## 负载均衡配置

```shell
#安装HAProxy和Keepalived（在所有Master节点上安装HAProxy和Keepalived）
yum -y install haproxy keepalived

#在所有Master节点上创建HAProxy配置文件：
cat > /etc/haproxy/haproxy.cfg << EOF
global
    log         127.0.0.1 local2
    chroot      /var/lib/haproxy
    pidfile     /var/run/haproxy.pid
    maxconn     4000
    user        haproxy
    group       haproxy
    daemon
    stats socket /var/lib/haproxy/stats

defaults
    mode                    tcp
    log                     global
    option                  tcplog
    option                  dontlognull
    option                  redispatch
    retries                 3
    timeout queue           1m
    timeout connect         10s
    timeout client          1m
    timeout server          1m
    timeout check           10s
    maxconn                 3000

frontend  k8s_https *:8443
    mode      tcp
    maxconn      2000
    default_backend     https_sri
    
backend https_sri
    balance      roundrobin
    server master1-api 192.168.1.20:6443  check inter 10000 fall 2 rise 2 weight 1
    server master2-api 192.168.1.21:6443  check inter 10000 fall 2 rise 2 weight 1
EOF

#在Master1节点上创建Keepalived配置文件：
cat > /etc/keepalived/keepalived.conf << EOF
global_defs {
   router_id LVS_DEVEL
}

vrrp_script check_haproxy {
    script "/etc/keepalived/check_haproxy.sh"
    interval 3000
}

vrrp_script check_nginx {
    script "/etc/keepalived/check_nginx.sh"
    interval 2
    weight -2
}

vrrp_instance VI_1 {
    state MASTER #来决定主从
    interface ens18 # 绑定虚拟 IP 的网络接口，根据自己的机器填写
    virtual_router_id 121 # 虚拟路由的 ID 号， 两个节点设置必须一样
    mcast_src_ip 192.168.1.20 #填写本机ip
    priority 100 # 节点优先级,主要比从节点优先级高
    nopreempt # 优先级高的设置 nopreempt 解决异常恢复后再次抢占的问题
    advert_int 1
    authentication {
        auth_type PASS
        auth_pass 1111
    }
    # 将 track_script 块加入 instance 配置块
    track_script {
        check_nginx
        check_haproxy
    }

    virtual_ipaddress {
        192.168.1.10
    }
}
EOF

#在Master2节点上创建Keepalived配置文件
cat > /etc/keepalived/keepalived.conf << EOF
global_defs {
   router_id LVS_DEVEL
}

vrrp_script check_haproxy {
    script "/etc/keepalived/check_haproxy.sh"
    interval 3000
}

vrrp_script check_nginx {
    script "/etc/keepalived/check_nginx.sh"
    interval 2
    weight -2
}

vrrp_instance VI_1 {
    state MASTER #来决定主从
    interface ens18 # 绑定虚拟 IP 的网络接口，根据自己的机器填写
    virtual_router_id 121 # 虚拟路由的 ID 号， 两个节点设置必须一样
    mcast_src_ip 192.168.1.21 #填写本机ip
    priority 90 # 节点优先级,主要比从节点优先级高
    nopreempt # 优先级高的设置 nopreempt 解决异常恢复后再次抢占的问题
    advert_int 1
    authentication {
        auth_type PASS
        auth_pass 1111
    }
    # 将 track_script 块加入 instance 配置块
    track_script {
        check_nginx
        check_haproxy
    }

    virtual_ipaddress {
        192.168.1.10
    }
}
EOF

#在所有Master节点上创建HAProxy检查脚本
cat > /etc/keepalived/check_haproxy.sh << EOF
#!/bin/bash
if [ `ps -C haproxy --no-header | wc -l` == 0 ]; then
        systemctl start haproxy
        sleep 3
        if [ `ps -C haproxy --no-header | wc -l` == 0 ]; then
                systemctl stop keepalived
        fi
fi
EOF
#添加可执行权限
chmod +x /etc/keepalived/check_haproxy.sh
#在所有Master节点上启动HAProxy和Keepalived，并设置自启动
systemctl start haproxy keepalived
systemctl enable haproxy keepalived
systemctl status haproxy keepalived
```

## 安装k8s

```shell
#添加kubernetes阿里YUM源
cat > /etc/yum.repos.d/kubernetes.repo << EOF
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
enabled=1
gpgcheck=0
repo_gpgcheck=0
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF
#所有节点安装kubectl、kubelet、kubeadm并设置开机启动
yum -y install kubelet-1.23.9 kubeadm-1.23.9 kubectl-1.23.9 && systemctl enable kubelet && systemctl start kubelet

#由于镜像在google在Registry上，国内无法访问，需要手动从阿里云或其他Registry上下载
kubeadm config images list --kubernetes-version 1.23.9
#在所有Master节点上下载镜像
kubeadm config images list --kubernetes-version 1.23.9 | sed -e 's/^/docker pull /g' -e 's#k8s.gcr.io#registry.aliyuncs.com/google_containers#g' | sh -x

#这时候注意，coredns的tag在阿里云是1.8.6，而google的是v1.8.6,所以手动下载，下载完打上官方镜像tag
docker tag registry.aliyuncs.com/google_containers/kube-apiserver:v1.23.9 k8s.gcr.io/kube-apiserver:v1.23.9
docker tag registry.aliyuncs.com/google_containers/kube-controller-manager:v1.23.9 k8s.gcr.io/kube-controller-manager:v1.23.9
docker tag registry.aliyuncs.com/google_containers/kube-scheduler:v1.23.9 k8s.gcr.io/kube-scheduler:v1.23.9
docker tag registry.aliyuncs.com/google_containers/kube-proxy:v1.23.9 k8s.gcr.io/kube-proxy:v1.23.9
docker tag registry.aliyuncs.com/google_containers/pause:3.6 k8s.gcr.io/pause:3.6
docker tag registry.aliyuncs.com/google_containers/etcd:3.5.1-0 k8s.gcr.io/etcd:3.5.1-0
docker tag registry.aliyuncs.com/google_containers/coredns:1.8.6 k8s.gcr.io/coredns/coredns:v1.8.6
```

初始化高可用集群

```shell
#在master上做免密登录
ssh-keygen
for host in master1 master2; do ssh-copy-id -i ~/.ssh/id_rsa.pub $host; done

#在Master1节点上创建集群配置文件：
cat > /etc/kubernetes/kubeadm-config.yaml << EOF
apiVersion: kubeadm.k8s.io/v1beta2
kind: ClusterConfiguration
kubernetesVersion: v1.23.9
controlPlaneEndpoint: "192.168.1.10:6443"
apiServer:
  certSANs:
  - 192.168.1.20
  - 192.168.1.21
  - 192.168.1.10
networking:
  podSubnet: 10.244.0.0/16
EOF

#在Master节点上初始化高可用集群：
kubeadm init --config /etc/kubernetes/kubeadm-config.yaml
#在Master1节点上拷贝证书至其余所有Master
for node in master2; do
  ssh $node "mkdir -p /etc/kubernetes/pki/etcd; mkdir -p ~/.kube/"
  scp /etc/kubernetes/pki/ca.crt $node:/etc/kubernetes/pki/ca.crt
  scp /etc/kubernetes/pki/ca.key $node:/etc/kubernetes/pki/ca.key
  scp /etc/kubernetes/pki/sa.key $node:/etc/kubernetes/pki/sa.key
  scp /etc/kubernetes/pki/sa.pub $node:/etc/kubernetes/pki/sa.pub
  scp /etc/kubernetes/pki/front-proxy-ca.crt $node:/etc/kubernetes/pki/front-proxy-ca.crt
  scp /etc/kubernetes/pki/front-proxy-ca.key $node:/etc/kubernetes/pki/front-proxy-ca.key
  scp /etc/kubernetes/pki/etcd/ca.crt $node:/etc/kubernetes/pki/etcd/ca.crt
  scp /etc/kubernetes/pki/etcd/ca.key $node:/etc/kubernetes/pki/etcd/ca.key
  scp /etc/kubernetes/admin.conf $node:/etc/kubernetes/admin.conf
  scp /etc/kubernetes/admin.conf $node:~/.kube/config
done
#将其余Master加入高可用集群：
kubeadm join 192.168.1.10:8443 --token 27dnp2.641fanafm9losc6g \
        --discovery-token-ca-cert-hash sha256:ebd0bb55bf65ab2892d5eb51ff775d70aaab00e863eb8779e96ca8e982d34fe1 \
        --control-plane
#将node节点加入master
kubeadm join 192.168.1.10:8443 --token 27dnp2.641fanafm9losc6g \
        --discovery-token-ca-cert-hash sha256:ebd0bb55bf65ab2892d5eb51ff775d70aaab00e863eb8779e96ca8e982d34fe1

#安装网络
wget https://docs.projectcalico.org/manifests/calico.yaml
#calico.yaml添加网卡信息
# Cluster type to identify the deployment type
  - name: CLUSTER_TYPE
  value: "k8s,bgp"
# 下方熙增新增
  - name: IP_AUTODETECTION_METHOD
    value: "interface=ens192"
    # ens192为本地网卡名字

##执行文件生成网络
kubectl apply -f calico.yaml

```
